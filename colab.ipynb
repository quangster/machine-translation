{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHXF26ZJpSaN",
    "outputId": "812dd3c0-d37c-46b6-b8b8-f9d166340d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'machine-translation'...\n",
      "remote: Enumerating objects: 22, done.\u001b[K\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 22 (delta 1), reused 19 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (22/22), 5.98 MiB | 5.75 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n",
      "/content/machine-translation\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/quangster/machine-translation\n",
    "%cd machine-translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvMbLXwTp0eg",
    "outputId": "a661326b-341d-4d5f-83a0-7113cf7e4069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 1OEYSMb7DHvhpHDkyErFfepIjeyDOiDR4 dev\n",
      "Processing file 1Koyp92dplbh_S_9UW8wvskbzHW6Gb5Zw dev.en\n",
      "Processing file 1KVzIWM8IUIS_NdWpctOd_l3FIm901e6L dev.vi\n",
      "Retrieving folder 1FkG-m-LSXaXCrau3yD8s9_f8Llda3KoF test\n",
      "Processing file 18XurJYc9T8i4JKzGRknNIMzEBD5bLDex test.en\n",
      "Processing file 1atCidgee403dxm8mAWIXq9mlfdcYSXc_ test.vi\n",
      "Retrieving folder 1jrfK8TmZghXISDq7JI-LTZyItZRgnEn2 train\n",
      "Processing file 1jR128Bdo7vyQc1OPBCE6zEz0gXF6eUDY train.en\n",
      "Processing file 1hKt2ww1-zZHzXRPl_0ijxUxdWK57XKp1 train.vi\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Koyp92dplbh_S_9UW8wvskbzHW6Gb5Zw\n",
      "To: /content/machine-translation/data/dev/dev.en\n",
      "100% 1.42M/1.42M [00:00<00:00, 124MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KVzIWM8IUIS_NdWpctOd_l3FIm901e6L\n",
      "To: /content/machine-translation/data/dev/dev.vi\n",
      "100% 1.89M/1.89M [00:00<00:00, 48.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=18XurJYc9T8i4JKzGRknNIMzEBD5bLDex\n",
      "To: /content/machine-translation/data/test/test.en\n",
      "100% 1.55M/1.55M [00:00<00:00, 139MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1atCidgee403dxm8mAWIXq9mlfdcYSXc_\n",
      "To: /content/machine-translation/data/test/test.vi\n",
      "100% 2.06M/2.06M [00:00<00:00, 128MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1jR128Bdo7vyQc1OPBCE6zEz0gXF6eUDY\n",
      "From (redirected): https://drive.google.com/uc?id=1jR128Bdo7vyQc1OPBCE6zEz0gXF6eUDY&confirm=t&uuid=b9ffe730-e00d-4c6b-adaa-73c37eb3ca0e\n",
      "To: /content/machine-translation/data/train/train.en\n",
      "100% 230M/230M [00:02<00:00, 96.2MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1hKt2ww1-zZHzXRPl_0ijxUxdWK57XKp1\n",
      "From (redirected): https://drive.google.com/uc?id=1hKt2ww1-zZHzXRPl_0ijxUxdWK57XKp1&confirm=t&uuid=31be3fce-165a-434f-bba8-1b35e6483c56\n",
      "To: /content/machine-translation/data/train/train.vi\n",
      "100% 302M/302M [00:02<00:00, 151MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!python setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UTPYWNb6tVwE"
   },
   "outputs": [],
   "source": [
    "from src.data import Vocabulary, EnTokenizer, ViTokenizer, MTDataset\n",
    "from src.utils.data import read_corpus\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý khi tokenize: vì các bộ vocab được build bằng chữ thường nên lúc tokenize cũng cần chuyển về chữ thường bằng hàm .lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['đảm', 'bảo', 'chất', 'lượng', 'phòng', ',', 'thí', 'nghiệm', 'hóa', 'học']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vietnamese tokenizer\n",
    "vi_tokenizer = ViTokenizer()\n",
    "vi_tokenizer.tokenize(\"   Ðảm baỏ chất lựơng phòng  , thí nghịêm       hoá học\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world', '!', ',', 'i', \"'m\", 'a', 'student', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english tokenizer\n",
    "en_tokenizer = EnTokenizer()\n",
    "en_tokenizer.tokenize(\"Hello,     world!, I'm a student.\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary[language=english, size=34687]\n",
      "Vocabulary[language=vietnamese, size=21681]\n"
     ]
    }
   ],
   "source": [
    "en_vocab = Vocabulary.load('./ckpts/en_vocab.json')\n",
    "vi_vocab = Vocabulary.load('./ckpts/vi_vocab.json')\n",
    "print(en_vocab)\n",
    "print(vi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    vi_tokenizer = ViTokenizer()\n",
    "    en_tokenizer = EnTokenizer()\n",
    "\n",
    "    global en_vocab, vi_vocab\n",
    "\n",
    "    train_en_sents, train_vi_sents = read_corpus(\"./data\", \"train\")\n",
    "    train_en_sents = train_en_sents[1300000:1400000]\n",
    "    train_vi_sents = train_vi_sents[1300000:1400000]\n",
    "\n",
    "    train_en_sents = [en_tokenizer.tokenize(sent.lower()) for sent in tqdm(train_en_sents)]\n",
    "    train_vi_sents = [vi_tokenizer.tokenize(sent.lower()) for sent in tqdm(train_vi_sents)]\n",
    "    \n",
    "    train_dataset = MTDataset(\n",
    "        inputs=[en_vocab.words2indexes(sent, add_sos_eos=True) for sent in train_en_sents],\n",
    "        outputs=[vi_vocab.words2indexes(sent, add_sos_eos=True) for sent in train_vi_sents],\n",
    "        max_length=20,\n",
    "        padding_idx=en_vocab['<pad>'],\n",
    "    )\n",
    "\n",
    "    val_en_sents, val_vi_sents = read_corpus(\"./data\", \"dev\")\n",
    "    val_en_sents = [en_tokenizer.tokenize(sent.lower()) for sent in tqdm(val_en_sents)]\n",
    "    val_vi_sents = [vi_tokenizer.tokenize(sent.lower()) for sent in tqdm(val_vi_sents)]\n",
    "\n",
    "    val_dataset = MTDataset(\n",
    "        inputs=[en_vocab.words2indexes(sent, add_sos_eos=True) for sent in val_en_sents],\n",
    "        outputs=[vi_vocab.words2indexes(sent, add_sos_eos=True) for sent in val_vi_sents],\n",
    "        max_length=20,\n",
    "        padding_idx=en_vocab['<pad>'],\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quá trình build torch Dataset có lược bớt các câu dài hơn max_length nên sẽ có số lượng ít hơn so với inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:05<00:00, 18942.26it/s]\n",
      "100%|██████████| 100000/100000 [00:06<00:00, 16639.19it/s]\n",
      "100%|██████████| 18719/18719 [00:01<00:00, 15111.10it/s]\n",
      "100%|██████████| 18719/18719 [00:01<00:00, 10613.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90919, 11668)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = get_dataset()\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'how', 'far', 'are', 'we', 'planning', 'on', 'taking', 'this', ',', 'dude', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<sos>', 'cậu', 'định', 'đóng', 'giả', 'đến', 'khi', 'nào', ',', 'anh', 'bạn', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# double check\n",
    "X, y = train_dataset[10]\n",
    "\n",
    "print(en_vocab.indexes2words(X.numpy()))\n",
    "print(vi_vocab.indexes2words(y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
