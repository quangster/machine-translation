{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHXF26ZJpSaN",
    "outputId": "812dd3c0-d37c-46b6-b8b8-f9d166340d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'machine-translation'...\n",
      "remote: Enumerating objects: 22, done.\u001b[K\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 22 (delta 1), reused 19 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (22/22), 5.98 MiB | 5.75 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n",
      "/content/machine-translation\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/quangster/machine-translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd machine-translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gdown\n",
    "!gdown --folder 1cPdLNnTlsj3N1FE9x6_K608bCAaYaVGM -O data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UTPYWNb6tVwE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/quangster/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data import Vocabulary, EnTokenizer, ViTokenizer, MTDataset\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý khi tokenize: vì các bộ vocab được build bằng chữ thường nên lúc tokenize cũng cần chuyển về chữ thường bằng hàm .lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['đảm', 'bảo', 'chất', 'lượng', 'phòng', ',', 'thí', 'nghiệm', 'hóa', 'học']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vietnamese tokenizer\n",
    "vi_tokenizer = ViTokenizer()\n",
    "vi_tokenizer.tokenize(\"   Ðảm baỏ chất lựơng phòng  , thí nghịêm       hoá học\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world', '!', ',', 'i', \"'m\", 'a', 'student', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english tokenizer\n",
    "en_tokenizer = EnTokenizer()\n",
    "en_tokenizer.tokenize(\"Hello,     world!, I'm a student.\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary[language=english, size=34687]\n",
      "Vocabulary[language=vietnamese, size=21681]\n"
     ]
    }
   ],
   "source": [
    "en_vocab = Vocabulary.load('./ckpts/en_vocab.json')\n",
    "vi_vocab = Vocabulary.load('./ckpts/vi_vocab.json')\n",
    "print(en_vocab)\n",
    "print(vi_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quá trình build dataset có lược bớt các câu dài hơn max_length, và loại bỏ các câu rỗng nên sẽ có số lượng ít hơn so với inputs nhận được"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    global en_vocab, vi_vocab\n",
    "\n",
    "    with open(\"./data/train.pkl\", \"rb\") as f:\n",
    "        train = pickle.load(f)\n",
    "        train_en_ids, train_vi_ids = train[0], train[1]\n",
    "\n",
    "    with open(\"./data/dev.pkl\", \"rb\") as f:\n",
    "        dev = pickle.load(f)\n",
    "        dev_en_ids, dev_vi_ids = dev[0], dev[1]\n",
    "\n",
    "    ####\n",
    "    # config length of training dataset or length of sequence here \n",
    "    #### \n",
    "\n",
    "    train_dataset = MTDataset(\n",
    "        train_en_ids[0:200000], \n",
    "        train_vi_ids[0:200000],\n",
    "        max_length=20,\n",
    "        padding_idx=en_vocab['<pad>'],\n",
    "    )\n",
    "    \n",
    "    val_dataset = MTDataset(\n",
    "        dev_en_ids, \n",
    "        dev_vi_ids,\n",
    "        max_length=20,\n",
    "        padding_idx=en_vocab['<pad>'],\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94104, 11668)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = get_dataset()\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'and', 'in', 'those', 'four', 'gestures', 'is', 'the', 'cycle', 'of', 'life', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<sos>', 'và', 'trong', '4', 'cử', 'chỉ', 'tay', 'này', 'vòng', 'xoay', 'của', 'cuộc', 'sống', 'được', 'tái', 'hiện', '.', '<eos>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# double check\n",
    "X, y = val_dataset[56]\n",
    "\n",
    "print(en_vocab.indexes2words(X.numpy()))\n",
    "print(vi_vocab.indexes2words(y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
