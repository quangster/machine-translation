# @package _global_

# to execute this experiment run:
# python src/train.py experiment=filter

defaults:
  - override /data: PhoMT
  - override /model: transformerv2
  - override /trainer: default
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 59
ckpt_path: null
train: True
test: True


# ------------------override configs/trainer------------------
trainer:
  min_epochs: 1 # prevents early stopping
  max_epochs: 10
  accelerator: gpu
  devices: 1
  log_every_n_steps: 100


# ------------------override configs/model------------------
model:
  net:
    src_vocab: 34687 # len(en_vocab)
    trg_vocab: 21681 # len(vi_vocab)
    d_model: 192
    N: 8 # num encoder and decoder layers
    heads: 6
    dropout: 0.1

  criterion:
    classes: 21681 # len(vi_vocab)
    padding_idx: 0
    smoothing: 0.1

  optimizer:
    _partial_: true
    lr: 0.01


# ------------------override configs/data------------------
data:
  train_split: [0, 500000]
  max_length: 50
  batch_size: 128
  num_workers: 4
  pin_memory: False


# ------------------override configs/logger------------------
logger:
  name: "transformer"
  project: "transformer"
  log_model: True # upload lightning ckpts
